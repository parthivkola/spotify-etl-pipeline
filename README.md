
# ğŸµ Spotify Data ETL Pipeline

## ğŸ“– *Overview*
A simple **ETL pipeline** in Python:
- **Extract** â†’ Read raw Kaggle CSV dataset  
- **Transform** â†’ Clean, handle missing values, add new columns  
- **Load** â†’ Save into PostgreSQL + local CSV  

This project demonstrates:
* Data cleaning with **pandas**  
* Secure credential handling with **dotenv**  
* Database integration with **SQLAlchemy + PostgreSQL**  

---

## ğŸ›  *Tech Stack*
* **Language** â†’ Python 3.11+  
* **Database** â†’ PostgreSQL  
* **Libraries** â†’ pandas, SQLAlchemy, psycopg2-binary, python-dotenv  

---

## âš™ï¸ *Setup*

### 1. Clone Repo
```bash
git clone https://github.com/parthivkola/spotify-etl-pipeline.git
cd spotify-etl-pipeline
```

### 2. Create Virtual Environment
```bash
conda create --name spotify_etl python=3.11
conda activate spotify_etl
pip install -r requirements.txt
```

### 3. Configure PostgreSQL
* Create DB â†’ `spotify_db`  
* Create user with permissions  

### 4. Configure Environment Variables
```bash
cp .env.example .env
```

Fill in:
```
DB_USER=your_username
DB_PASSWORD=your_password
DB_HOST=localhost
DB_PORT=5432
DB_NAME=spotify_db
```

---

## â–¶ï¸ *Run the Pipeline*
```bash
python main.py
```

Steps:
* Extract â†’ from `dataset.csv`  
* Transform â†’ clean + add `duration_min`  
* Load â†’ into Postgres (`popular_tracks`)  

Query data:
```sql
SELECT * FROM popular_tracks LIMIT 10;
```

---

## ğŸ“Š *Example Output*

| track_name | artist_name | duration_min | popularity |
|------------|-------------|--------------|-------------|
| Song A     | Artist 1    | 3.5          | 85          |
| Song B     | Artist 2    | 4.1          | 90          |

---
